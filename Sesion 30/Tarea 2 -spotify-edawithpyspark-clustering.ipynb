{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-14T15:11:17.124563Z",
     "iopub.status.busy": "2021-07-14T15:11:17.123857Z",
     "iopub.status.idle": "2021-07-14T15:11:23.730693Z",
     "shell.execute_reply": "2021-07-14T15:11:23.729611Z",
     "shell.execute_reply.started": "2021-07-14T15:11:17.124526Z"
    }
   },
   "source": [
    "This notebook serves as an example of using PySpark to explore big data, as well as explore the Spotify API functionality, and build a deep-embedding recommendation system. I have some good examples of SQL queries for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:13:09.771774Z",
     "iopub.status.busy": "2022-07-04T01:13:09.77101Z",
     "iopub.status.idle": "2022-07-04T01:13:16.647676Z",
     "shell.execute_reply": "2022-07-04T01:13:16.646377Z",
     "shell.execute_reply.started": "2022-07-04T01:13:09.771709Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyspark\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set_palette('rainbow')\n",
    "sns.set_style('whitegrid')\n",
    "import plotly.express as px\n",
    "# these 2 lines fix a sporatic loading error in plotly\n",
    "from plotly.offline import plot, iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "# pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "CSV_FILE= '/kaggle/input/spotify-huge-database-daily-charts-over-3-years/Final database.csv'\n",
    "# CSV_FILE= '/kaggle/input/spotify-huge-database-daily-charts-over-3-years/Database to calculate popularity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-07-04T01:13:16.649785Z",
     "iopub.status.busy": "2022-07-04T01:13:16.649431Z",
     "iopub.status.idle": "2022-07-04T01:13:16.654874Z",
     "shell.execute_reply": "2022-07-04T01:13:16.653411Z",
     "shell.execute_reply.started": "2022-07-04T01:13:16.64975Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From SparkByExample:\n",
    "> A spark session unifies all the different contexts, and you can access all the different contexts by invoking them on the spark session object. A Spark “driver” is an application that creates a SparkContext for executing one or more jobs in the Spark cluster. It allows your Spark/PySpark application to access Spark Cluster with the help of Resource Manager.\n",
    "> \n",
    "> When you create a SparkSession object, SparkContext is also created and can be retrieved using spark.sparkContext. SparkContext will be created only once for an application; even if you try to create another SparkContext, it still returns existing SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:13:16.656582Z",
     "iopub.status.busy": "2022-07-04T01:13:16.656136Z",
     "iopub.status.idle": "2022-07-04T01:13:16.673215Z",
     "shell.execute_reply": "2022-07-04T01:13:16.671936Z",
     "shell.execute_reply.started": "2022-07-04T01:13:16.656547Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Spotify-Huge-Dataset\").getOrCreate() #.enableHiveSupport()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:13:16.675417Z",
     "iopub.status.busy": "2022-07-04T01:13:16.674782Z",
     "iopub.status.idle": "2022-07-04T01:13:16.687194Z",
     "shell.execute_reply": "2022-07-04T01:13:16.685579Z",
     "shell.execute_reply.started": "2022-07-04T01:13:16.675371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Old way depreciated in 3.0.0\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note I'm fixing the numerical features after loading the df. This is much slower than defining the schema before loading into a spark dataframe. I'll come back and define the schema explicitely later when I have some free time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-07-04T01:13:16.689291Z",
     "iopub.status.busy": "2022-07-04T01:13:16.688664Z",
     "iopub.status.idle": "2022-07-04T01:15:49.086651Z",
     "shell.execute_reply": "2022-07-04T01:15:49.085756Z",
     "shell.execute_reply.started": "2022-07-04T01:13:16.689247Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(CSV_FILE)\n",
    "df = df.withColumn(\"Release_date\", F.to_date(\"Release_date\", \"yyyy-MM-dd\"))\n",
    "numerical_features = ['danceability', 'energy', 'instrumentalness', 'valence', 'liveliness', 'speechiness', 'acoustics',\n",
    "                      'instrumentalness', 'tempo', 'duration_ms', \n",
    "                      'time_signature', 'Days_since_release', 'n_words']\n",
    "\n",
    "for c in numerical_features:\n",
    "    df = df.withColumn(c, df[c].cast(\"float\"))\n",
    "    \n",
    "cols_to_drop = ['syuzhet_norm', 'bing_norm', 'afinn_norm', 'nrc_norm', 'syuzhet', 'bing'] \n",
    "for c in cols_to_drop:\n",
    "    df.drop(c).collect()\n",
    "    \n",
    "df.printSchema()\n",
    "# df.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Some good SQL queries, Plotly figures, and examples of using pyspark to filter results from a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:49.088315Z",
     "iopub.status.busy": "2022-07-04T01:15:49.08788Z",
     "iopub.status.idle": "2022-07-04T01:15:50.561751Z",
     "shell.execute_reply": "2022-07-04T01:15:50.560572Z",
     "shell.execute_reply.started": "2022-07-04T01:15:49.088283Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many unique songs are there in the dataset?\n",
    "df.select([\"Title\",\"Artist\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:50.568041Z",
     "iopub.status.busy": "2022-07-04T01:15:50.567215Z",
     "iopub.status.idle": "2022-07-04T01:15:50.943594Z",
     "shell.execute_reply": "2022-07-04T01:15:50.9426Z",
     "shell.execute_reply.started": "2022-07-04T01:15:50.567985Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Shape using spark syntax:\\n\",(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular artist, all countries. Each tally represents a song on a given day (during the last 3 days) that was one of the most 200 most played songs on that day. An artist can have multiple songs per day, and the same song can be counted on again on subsequent days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:50.945753Z",
     "iopub.status.busy": "2022-07-04T01:15:50.945334Z",
     "iopub.status.idle": "2022-07-04T01:15:51.992087Z",
     "shell.execute_reply": "2022-07-04T01:15:51.990724Z",
     "shell.execute_reply.started": "2022-07-04T01:15:50.945682Z"
    }
   },
   "outputs": [],
   "source": [
    "# each count is a song that was in the top 200 most played on a day on spotify during the last 3 years\n",
    "result_df = (df.groupBy(\"Artist\")\n",
    "               .count()\n",
    "               .orderBy(\"count\", ascending=False)\n",
    "               .limit(10)\n",
    "               .toPandas()\n",
    "            )\n",
    "px.bar(result_df, y='Artist', x='count', title='Most Prolific Artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:51.995074Z",
     "iopub.status.busy": "2022-07-04T01:15:51.994173Z",
     "iopub.status.idle": "2022-07-04T01:15:52.455065Z",
     "shell.execute_reply": "2022-07-04T01:15:52.454085Z",
     "shell.execute_reply.started": "2022-07-04T01:15:51.99501Z"
    }
   },
   "outputs": [],
   "source": [
    "# same as above but with seaborn (sometimes plotly doesn't show up in the published notebok)\n",
    "sns.barplot(data=result_df, y='Artist', x='count').set_title('Most Prolific Artists');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CreateOrReplaceTempView` will create a temporary view of the table on memory. It won't persist, but you can run SQL queries on top of it. You can always force it to cache/persits with `saveAsTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:52.457113Z",
     "iopub.status.busy": "2022-07-04T01:15:52.456427Z",
     "iopub.status.idle": "2022-07-04T01:15:52.479535Z",
     "shell.execute_reply": "2022-07-04T01:15:52.478462Z",
     "shell.execute_reply.started": "2022-07-04T01:15:52.457063Z"
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:52.481651Z",
     "iopub.status.busy": "2022-07-04T01:15:52.481073Z",
     "iopub.status.idle": "2022-07-04T01:15:52.863368Z",
     "shell.execute_reply": "2022-07-04T01:15:52.862214Z",
     "shell.execute_reply.started": "2022-07-04T01:15:52.48161Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Now using the SQL Context. We can check it's the same length as before\")\n",
    "query = \"\"\"\n",
    "    SELECT Count(*) as Dataset_Length\n",
    "    FROM df_table\n",
    "\"\"\"\n",
    "res = spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:52.865114Z",
     "iopub.status.busy": "2022-07-04T01:15:52.864759Z",
     "iopub.status.idle": "2022-07-04T01:15:54.149337Z",
     "shell.execute_reply": "2022-07-04T01:15:54.148019Z",
     "shell.execute_reply.started": "2022-07-04T01:15:52.865081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most popular artist (by sum of popularity of songs) in the USA\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "                                    Artist, \n",
    "       ROUND(SUM(Popularity), 2) AS Populartiy\n",
    "FROM df_table\n",
    "WHERE USA == 1\n",
    "GROUP BY Artist\n",
    "ORDER BY AVG(Popularity) DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(query)\n",
    "res.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:54.151512Z",
     "iopub.status.busy": "2022-07-04T01:15:54.151064Z",
     "iopub.status.idle": "2022-07-04T01:15:55.383963Z",
     "shell.execute_reply": "2022-07-04T01:15:55.382958Z",
     "shell.execute_reply.started": "2022-07-04T01:15:54.151463Z"
    }
   },
   "outputs": [],
   "source": [
    "# select only the songs released in 1939\n",
    "(df.filter(F.year(df['Release_date']) == 1939)\n",
    "   .select('Title', 'Artist','Release_date', 'Genre')\n",
    "   .distinct()\n",
    "   .show(5, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Song per Decade\n",
    "First with a nested query and using pandas to drop duplicates. Then optimized with [scalar-aggregate reduction](https://www.stevenmoseley.com/blog/tech/high-performance-sql-correlated-scalar-aggregate-reduction-queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:55.385461Z",
     "iopub.status.busy": "2022-07-04T01:15:55.385149Z",
     "iopub.status.idle": "2022-07-04T01:15:59.089529Z",
     "shell.execute_reply": "2022-07-04T01:15:59.087931Z",
     "shell.execute_reply.started": "2022-07-04T01:15:55.38543Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "        ROUND(Year(Release_date), -1) AS Decade,\n",
    "        Round(Popularity, 2)          AS Popularity,\n",
    "                                         Title,\n",
    "                                         Artist\n",
    "FROM df_table\n",
    "INNER JOIN (SELECT Max(Popularity) as mp\n",
    "            FROM df_table\n",
    "            WHERE ROUND(Year(Release_date), -1) IS NOT NULL\n",
    "            AND USA == 1\n",
    "            GROUP BY ROUND(Year(Release_date), -1)\n",
    "           ) AS temp\n",
    "ON temp.mp = df_table.Popularity\n",
    "ORDER BY Decade ASC, Popularity ASC\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(query)\n",
    "res.toPandas().drop_duplicates(subset='Decade', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:15:59.091266Z",
     "iopub.status.busy": "2022-07-04T01:15:59.090973Z",
     "iopub.status.idle": "2022-07-04T01:16:00.625307Z",
     "shell.execute_reply": "2022-07-04T01:16:00.624126Z",
     "shell.execute_reply.started": "2022-07-04T01:15:59.091236Z"
    }
   },
   "outputs": [],
   "source": [
    "# highly optimized version of the above query via scalar-aggregate-reduction\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ROUND(Year(Release_date), -1) as Decade,\n",
    "    ROUND(Max(Popularity), 2) as Popularity,\n",
    "    SUBSTRING(MAX(CONCAT(LPAD(Popularity, 11, 0), Title)), 12) AS Title,\n",
    "    SUBSTRING(MAX(CONCAT(LPAD(Popularity, 11, 0), Artist)), 12) AS Artist\n",
    "FROM\n",
    "    df_table\n",
    "WHERE\n",
    "    ROUND(Year(Release_date), -1) IS NOT NULL\n",
    "    AND USA == 1\n",
    "GROUP BY Decade\n",
    "ORDER BY Decade ASC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular Genre per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:00.627227Z",
     "iopub.status.busy": "2022-07-04T01:16:00.62677Z",
     "iopub.status.idle": "2022-07-04T01:16:01.602702Z",
     "shell.execute_reply": "2022-07-04T01:16:01.601899Z",
     "shell.execute_reply.started": "2022-07-04T01:16:00.627181Z"
    }
   },
   "outputs": [],
   "source": [
    " # Most popular genres, period.\n",
    "query = \"\"\"\n",
    "SELECT Genre, COUNT(*) AS Tally\n",
    "FROM df_table\n",
    "GROUP BY Genre\n",
    "ORDER BY Tally DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:01.604065Z",
     "iopub.status.busy": "2022-07-04T01:16:01.603733Z",
     "iopub.status.idle": "2022-07-04T01:16:03.568731Z",
     "shell.execute_reply": "2022-07-04T01:16:03.567499Z",
     "shell.execute_reply.started": "2022-07-04T01:16:01.604031Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "      ROUND(Year(Release_date), -1) AS Decade,\n",
    "      Genre, COUNT(Genre)           AS counts\n",
    "FROM  df_table\n",
    "WHERE ROUND(Year(Release_date), -1) IS NOT NULL\n",
    "GROUP BY Decade, Genre\n",
    "ORDER BY COUNT(Genre) DESC\n",
    "\"\"\"\n",
    "\n",
    "res = (spark.sql(query)\n",
    "            .dropDuplicates(subset=['Decade'])\n",
    "            .orderBy('Decade')\n",
    "            .show()\n",
    "      )\n",
    "# res.toPandas().drop_duplicates(subset='Decade', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each track, what day was it most popular?\n",
    "(Just for a small selection of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:03.571557Z",
     "iopub.status.busy": "2022-07-04T01:16:03.571167Z",
     "iopub.status.idle": "2022-07-04T01:16:04.640565Z",
     "shell.execute_reply": "2022-07-04T01:16:04.639183Z",
     "shell.execute_reply.started": "2022-07-04T01:16:03.57152Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Title, Artist, Release_date, MAX(Popularity)\n",
    "FROM df_table\n",
    "WHERE Artist == \"Paulo Londra\"\n",
    "GROUP BY Title, Artist, Release_date\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "res = spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how music changed over the decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:04.642377Z",
     "iopub.status.busy": "2022-07-04T01:16:04.641967Z",
     "iopub.status.idle": "2022-07-04T01:16:06.296095Z",
     "shell.execute_reply": "2022-07-04T01:16:06.294955Z",
     "shell.execute_reply.started": "2022-07-04T01:16:04.642324Z"
    }
   },
   "outputs": [],
   "source": [
    "sound_features = ['danceability', 'energy', 'instrumentalness', 'valence', 'liveliness', 'speechiness', 'acoustics']\n",
    "col_names = ['Decade']\n",
    "col_names.extend(sound_features)\n",
    "\n",
    "df_music_features = (df.sample(.2, seed=42)\n",
    "                       .groupBy(F.round(F.year(df.Release_date), -1))\n",
    "                       .agg({feature: 'mean' for feature in sound_features})\n",
    "                       .toDF(*col_names)\n",
    "                       .orderBy('Decade')\n",
    "                       .toPandas()\n",
    "                       .dropna(axis=0)\n",
    "                    )\n",
    "fig = px.line(df_music_features, x='Decade', y=sound_features, title='Song Characteristics Over the Decades')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:06.298571Z",
     "iopub.status.busy": "2022-07-04T01:16:06.297866Z",
     "iopub.status.idle": "2022-07-04T01:16:06.721293Z",
     "shell.execute_reply": "2022-07-04T01:16:06.719915Z",
     "shell.execute_reply.started": "2022-07-04T01:16:06.298515Z"
    }
   },
   "outputs": [],
   "source": [
    "# same as above but with seaborn. (sometimes plotly doesn't show up in the published notebok)\n",
    "sns.lineplot(data=pd.melt(df_music_features, ['Decade']), x='Decade', y='value', hue='variable').set_title('Song Characteristics Over the Decades');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check out the spotify API\n",
    "\n",
    "[currently based off this](https://www.kaggle.com/vatsalmavani/music-recommendation-system-using-spotify-dataset). We can extract more song information than is provided by the dataset by interacting with the Spotify API. Using this, we can get features like song length using `spotipy.audio_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:06.723539Z",
     "iopub.status.busy": "2022-07-04T01:16:06.723044Z",
     "iopub.status.idle": "2022-07-04T01:16:07.225165Z",
     "shell.execute_reply": "2022-07-04T01:16:07.224102Z",
     "shell.execute_reply.started": "2022-07-04T01:16:06.723485Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "SPOTIFY_CLIENT_ID = user_secrets.get_secret(\"SPOTIFY_CLIENT_ID\")\n",
    "SPOTIFY_CLIENT_SECRET = user_secrets.get_secret(\"SPOTIFY_CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:07.226917Z",
     "iopub.status.busy": "2022-07-04T01:16:07.226562Z",
     "iopub.status.idle": "2022-07-04T01:16:14.085515Z",
     "shell.execute_reply": "2022-07-04T01:16:14.083798Z",
     "shell.execute_reply.started": "2022-07-04T01:16:07.226879Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:14.089735Z",
     "iopub.status.busy": "2022-07-04T01:16:14.089289Z",
     "iopub.status.idle": "2022-07-04T01:16:14.10135Z",
     "shell.execute_reply": "2022-07-04T01:16:14.100014Z",
     "shell.execute_reply.started": "2022-07-04T01:16:14.089695Z"
    }
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from collections import defaultdict\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=SPOTIFY_CLIENT_ID,\n",
    "                                                           client_secret=SPOTIFY_CLIENT_SECRET\n",
    "                                                          )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:14.104916Z",
     "iopub.status.busy": "2022-07-04T01:16:14.10408Z",
     "iopub.status.idle": "2022-07-04T01:16:14.425688Z",
     "shell.execute_reply": "2022-07-04T01:16:14.424505Z",
     "shell.execute_reply.started": "2022-07-04T01:16:14.104856Z"
    }
   },
   "outputs": [],
   "source": [
    "# to search for a specific song title and filter the returned JSON\n",
    "sp.search(q='track: smells like teen spirit')['tracks']['items'][0]['album']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:14.427487Z",
     "iopub.status.busy": "2022-07-04T01:16:14.427165Z",
     "iopub.status.idle": "2022-07-04T01:16:14.437628Z",
     "shell.execute_reply": "2022-07-04T01:16:14.436358Z",
     "shell.execute_reply.started": "2022-07-04T01:16:14.427454Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_song(name, year):\n",
    "    song_data = defaultdict()\n",
    "    results = sp.search(q=f'track: {name} year: {year}', limit=1)\n",
    "    if results['tracks']['items'] == []:\n",
    "        return None\n",
    "\n",
    "    results = results['tracks']['items'][0]\n",
    "    track_id = results['id']\n",
    "    audio_features = sp.audio_features(track_id)[0]\n",
    "\n",
    "    song_data['name'] = [name]\n",
    "    song_data['year'] = [year]\n",
    "    song_data['explicit'] = [int(results['explicit'])]\n",
    "    song_data['duration_ms'] = [results['duration_ms']]\n",
    "    song_data['popularity'] = [results['popularity']]\n",
    "\n",
    "    for key, value in audio_features.items():\n",
    "        song_data[key] = value\n",
    "\n",
    "    return pd.DataFrame(song_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few possible approaches for comparing song similarities. One is to just use the continuous, numerical variables (things like danceability, energy, etc.) and do PCA or k-means or some other way to reduce dimensionality.  If you're just considering the song features (continuous variables) you could just create a feature vector and look at the cosine similartity to find the most similar sounding song, taking into account the numerical features and the one-hot-encoded countries.\n",
    "\n",
    "Some options: \n",
    "- [Non-linear PCA (NLPCA)](https://pubmed.ncbi.nlm.nih.gov/22176263/)\n",
    "- [Factor Analysis of Mixed Data (FAMD)](https://github.com/MaxHalford/Prince#factor-analysis-of-mixed-data-famd)\n",
    "\n",
    "Alternatively, we can create an embedding, where we map all the songs into an n-dimensional feature space and then look for the most similar vectors in this space (probably with k-NNN. Then we can get the k-most similar songs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First check out the cosine similiarty of song feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:14.440431Z",
     "iopub.status.busy": "2022-07-04T01:16:14.439616Z",
     "iopub.status.idle": "2022-07-04T01:16:16.997833Z",
     "shell.execute_reply": "2022-07-04T01:16:16.996616Z",
     "shell.execute_reply.started": "2022-07-04T01:16:14.440372Z"
    }
   },
   "outputs": [],
   "source": [
    "###### First KPop ######\n",
    "query_kpop = \"\"\"\n",
    "SELECT Title, Artist, {}\n",
    "FROM df_table\n",
    "WHERE `k-pop` = 1\n",
    "\"\"\".format(', '.join(numerical_features))\n",
    "\n",
    "df_kpop_songs = (spark.sql(query_kpop)\n",
    "                      .sample(.1)\n",
    "                      .dropna()\n",
    "                      .toPandas() # don't do this, it's better to sample before querying\n",
    "                )\n",
    "####### Now Rap #######\n",
    "query_rap = \"\"\"\n",
    "SELECT Title, Artist, {}\n",
    "FROM df_table\n",
    "WHERE rap = 1\n",
    "\"\"\".format(', '.join(numerical_features))\n",
    "\n",
    "df_rap_songs = (spark.sql(query_rap)\n",
    "                     .sample(.1)\n",
    "                     .dropna()\n",
    "                     .toPandas() # don't do this, it's better to sample before querying\n",
    "               )\n",
    "df_rap_songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:16.999495Z",
     "iopub.status.busy": "2022-07-04T01:16:16.999176Z",
     "iopub.status.idle": "2022-07-04T01:16:17.029223Z",
     "shell.execute_reply": "2022-07-04T01:16:17.027969Z",
     "shell.execute_reply.started": "2022-07-04T01:16:16.999464Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kpop_songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.031148Z",
     "iopub.status.busy": "2022-07-04T01:16:17.030771Z",
     "iopub.status.idle": "2022-07-04T01:16:17.054214Z",
     "shell.execute_reply": "2022-07-04T01:16:17.053105Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.03111Z"
    }
   },
   "outputs": [],
   "source": [
    "# it might be better to used a normalized cosine similarity instead of scaling first and then doing it.\n",
    "from scipy import spatial\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "transformer = Normalizer()\n",
    "\n",
    "scaled_kpop_df = scaler.fit_transform(df_kpop_songs.iloc[:, 2:]) # drop the title and artist with the iloc\n",
    "scaled_rap_df = scaler.fit_transform(df_rap_songs.iloc[:, 2:])\n",
    "\n",
    "\n",
    "# cos similarity of a rap and a k-pop song\n",
    "song1 = np.array(scaled_rap_df[1])\n",
    "song2 = np.array(scaled_kpop_df[2])\n",
    "result = 1 - spatial.distance.cosine(song1, song2)\n",
    "print(\"Cosine similarity of a rap and a k-pop song:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.056518Z",
     "iopub.status.busy": "2022-07-04T01:16:17.055891Z",
     "iopub.status.idle": "2022-07-04T01:16:17.064045Z",
     "shell.execute_reply": "2022-07-04T01:16:17.062899Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.056468Z"
    }
   },
   "outputs": [],
   "source": [
    "# cos similarity of two rap songs\n",
    "song1 = np.array(scaled_rap_df[1])\n",
    "song2 = np.array(scaled_rap_df[10])\n",
    "result = 1 - spatial.distance.cosine(song1, song2)\n",
    "print(\"Cosine similarity of two rap songs:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimentionality reduction\n",
    "This is useful for visualizing kmeans clustering later. One mistake people make with PCA, is assuming that the dimentions you get will be interpretable. In this case, we're going to take two types of music (Kpop and Rap), and then try reducing all the numeric, musical features down to two dimentions. The two dimentions won't really represent the genre of the music, but we can pretend that this is true. When we do KMeans clustering later on, we can visualize it on these two PCA axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.066066Z",
     "iopub.status.busy": "2022-07-04T01:16:17.065562Z",
     "iopub.status.idle": "2022-07-04T01:16:17.106199Z",
     "shell.execute_reply": "2022-07-04T01:16:17.104914Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.066017Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's add an OHE genre encoding\n",
    "df_rap_songs = df_rap_songs.assign(is_rap=1,\n",
    "                                   is_kpop=0\n",
    "                                   )\n",
    "df_kpop_songs = df_kpop_songs.assign(is_rap=0,\n",
    "                                     is_kpop=1\n",
    "                                     )\n",
    "df_rap_and_kpop = pd.concat([df_rap_songs, df_kpop_songs])\n",
    "X = scaler.fit_transform(df_rap_and_kpop.iloc[:, 2:])\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.115944Z",
     "iopub.status.busy": "2022-07-04T01:16:17.112584Z",
     "iopub.status.idle": "2022-07-04T01:16:17.386497Z",
     "shell.execute_reply": "2022-07-04T01:16:17.385215Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.115869Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=[x for x in range(1, 11)], y=pca.explained_variance_ratio_).set_title(\"% Variance Explained vs # Dimensions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it's able to explain most the vairance using 1 dimension. This roughly corresponds to \"genre,\" which instead was encoded as either `is_rap` or `is_kpop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.388784Z",
     "iopub.status.busy": "2022-07-04T01:16:17.388324Z",
     "iopub.status.idle": "2022-07-04T01:16:17.747369Z",
     "shell.execute_reply": "2022-07-04T01:16:17.746081Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.388735Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "PCA_components = pd.DataFrame(principalComponents)\n",
    "\n",
    "# sns.scatterplot(data=principalComponents, alpha=.1)\n",
    "sns.scatterplot(x=PCA_components[0], y=PCA_components[1], alpha=.1).set_title(\"First 2 PCA Components\");\n",
    "plt.xlabel('PCA 1');\n",
    "plt.ylabel('PCA 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component is particularly excellent at separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimum number of clusters with an elbow plot. View the top 2 PCA clusters, and then use kmeans with various number of clusters. The \"Scree\" plot below, shows the percent of variance explained as a function of the number of clusters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:17.749273Z",
     "iopub.status.busy": "2022-07-04T01:16:17.748907Z",
     "iopub.status.idle": "2022-07-04T01:16:21.338217Z",
     "shell.execute_reply": "2022-07-04T01:16:21.337155Z",
     "shell.execute_reply.started": "2022-07-04T01:16:17.749232Z"
    }
   },
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:2])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "sns.lineplot(x=ks, y=inertias, marker='o').set_title(\"Inertia vs # Clusters used\")\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as expected, 2 clusters seems to make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:21.340122Z",
     "iopub.status.busy": "2022-07-04T01:16:21.339728Z",
     "iopub.status.idle": "2022-07-04T01:16:21.374575Z",
     "shell.execute_reply": "2022-07-04T01:16:21.373423Z",
     "shell.execute_reply.started": "2022-07-04T01:16:21.340084Z"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=2, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:21.37956Z",
     "iopub.status.busy": "2022-07-04T01:16:21.379157Z",
     "iopub.status.idle": "2022-07-04T01:16:21.38989Z",
     "shell.execute_reply": "2022-07-04T01:16:21.388966Z",
     "shell.execute_reply.started": "2022-07-04T01:16:21.37952Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pca_kmeans_plot = pd.concat([PCA_components, pd.Series(y_km)], axis=1)\n",
    "df_pca_kmeans_plot.columns = ['PCA_1', 'PCA_2', 'Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:21.395982Z",
     "iopub.status.busy": "2022-07-04T01:16:21.393963Z",
     "iopub.status.idle": "2022-07-04T01:16:21.857919Z",
     "shell.execute_reply": "2022-07-04T01:16:21.856733Z",
     "shell.execute_reply.started": "2022-07-04T01:16:21.39593Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_pca_kmeans_plot, x='PCA_1', y='PCA_2', hue='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2 component PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:21.860257Z",
     "iopub.status.busy": "2022-07-04T01:16:21.859589Z",
     "iopub.status.idle": "2022-07-04T01:16:22.331898Z",
     "shell.execute_reply": "2022-07-04T01:16:22.330694Z",
     "shell.execute_reply.started": "2022-07-04T01:16:21.860204Z"
    }
   },
   "outputs": [],
   "source": [
    "# now color by genre instead of cluster\n",
    "df_final = pd.concat([df_pca_kmeans_plot, df_rap_and_kpop.reset_index()['is_rap']], axis=1)\n",
    "df_final['is_rap'] = df_final['is_rap'].replace({1:'Rap', 0: 'KPop'})\n",
    "df_final['Cluster'] = df_final['Cluster'].replace({1:'Cluster 2', 0: 'Cluster 1'})\n",
    "df_final = df_final.rename(columns={'is_rap': 'Genre'})\n",
    "\n",
    "sns.scatterplot(data=df_final, x='PCA_1', y='PCA_2', hue='Genre')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2 component PCA');\n",
    "# px.scatter(df_final, x='PCA_1', y='PCA_2', color='Genre', hover_data=['Genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, would it work it be able to pick out genre and group into two clusters as nicely, if we excluded the \"Genre\" features from the training labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:22.333938Z",
     "iopub.status.busy": "2022-07-04T01:16:22.333558Z",
     "iopub.status.idle": "2022-07-04T01:16:24.502053Z",
     "shell.execute_reply": "2022-07-04T01:16:24.50099Z",
     "shell.execute_reply.started": "2022-07-04T01:16:22.333899Z"
    }
   },
   "outputs": [],
   "source": [
    "df_rap_and_kpop = pd.concat([df_rap_songs, df_kpop_songs])\n",
    "rap_kpop_labels = df_rap_and_kpop[['is_rap', 'is_kpop']]\n",
    "df_rap_and_kpop = df_rap_and_kpop.drop(columns=['is_rap', 'is_kpop'])\n",
    "X = scaler.fit_transform(df_rap_and_kpop.iloc[:, 2:])\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "sns.lineplot(x=[x for x in range(1, 11)], y=pca.explained_variance_ratio_).set_title(\"% Variance Explained vs # Dimensions\")\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "PCA_components = pd.DataFrame(principalComponents)\n",
    "\n",
    "# sns.scatterplot(data=principalComponents, alpha=.1)\n",
    "sns.scatterplot(x=PCA_components[0], y=PCA_components[1], alpha=.1).set_title(\"First 2 PCA Components\");\n",
    "plt.xlabel('PCA 1');\n",
    "plt.ylabel('PCA 2');\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:2])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "sns.lineplot(x=ks, y=inertias, marker='o').set_title(\"Inertia vs # Clusters used\")\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=3, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "df_pca_kmeans_plot = pd.concat([PCA_components, pd.Series(y_km)], axis=1)\n",
    "df_pca_kmeans_plot.columns = ['PCA_1', 'PCA_2', 'Cluster']\n",
    "sns.scatterplot(data=df_pca_kmeans_plot, x='PCA_1', y='PCA_2', hue='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2 component PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:24.504021Z",
     "iopub.status.busy": "2022-07-04T01:16:24.503631Z",
     "iopub.status.idle": "2022-07-04T01:16:24.951221Z",
     "shell.execute_reply": "2022-07-04T01:16:24.948574Z",
     "shell.execute_reply.started": "2022-07-04T01:16:24.503982Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_pca_kmeans_plot, rap_kpop_labels.reset_index()['is_rap']], axis=1)\n",
    "df_final['is_rap'] = df_final['is_rap'].replace({1:'Rap', 0: 'KPop'})\n",
    "df_final['Cluster'] = df_final['Cluster'].replace({1:'Cluster 2', 0: 'Cluster 1'})\n",
    "df_final = df_final.rename(columns={'is_rap': 'Genre'})\n",
    "\n",
    "# px.scatter(df_final, x='PCA_1', y='PCA_2', color='Genre', hover_data=['Cluster'])\n",
    "sns.scatterplot(data=df_final, x='PCA_1', y='PCA_2', hue='Genre')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2 component PCA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without providing PCA the genre as a feature, it separates the data very differently. You can see that it makes no attempt to separate by genre (when we add it back in and plot, coloring by genre instead of cluster). And interestingly, it suggests that three clusters is the best separation, instead of two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most similar song\n",
    "We could do something like fit_transform the entire dataset, then write a function to loop through all the poosible songs (only like 60k of them), and then return the minimum. We need to be careful or at least selective about the features though, because trying to OHE all the variables might exceed our RAM limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:24.953545Z",
     "iopub.status.busy": "2022-07-04T01:16:24.953033Z",
     "iopub.status.idle": "2022-07-04T01:16:28.136115Z",
     "shell.execute_reply": "2022-07-04T01:16:28.134959Z",
     "shell.execute_reply.started": "2022-07-04T01:16:24.953495Z"
    }
   },
   "outputs": [],
   "source": [
    "# we might want to grab the URI too, to compare how they sound later. I'll skip for now\n",
    "query_all = \"\"\"\n",
    "SELECT Title, Artist, Genre, {}\n",
    "FROM df_table\n",
    "\"\"\".format(', '.join(numerical_features))\n",
    "\n",
    "df_all_songs = (spark.sql(query_all)\n",
    "                     .dropna()\n",
    "                     .toPandas()\n",
    "                     .drop_duplicates(['Title', 'Artist'])\n",
    "                     .reset_index(drop=True)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:28.137889Z",
     "iopub.status.busy": "2022-07-04T01:16:28.137544Z",
     "iopub.status.idle": "2022-07-04T01:16:28.147511Z",
     "shell.execute_reply": "2022-07-04T01:16:28.146256Z",
     "shell.execute_reply.started": "2022-07-04T01:16:28.137856Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all_songs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:28.149261Z",
     "iopub.status.busy": "2022-07-04T01:16:28.148941Z",
     "iopub.status.idle": "2022-07-04T01:16:29.274545Z",
     "shell.execute_reply": "2022-07-04T01:16:29.27341Z",
     "shell.execute_reply.started": "2022-07-04T01:16:28.14923Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all_songs_ohe = pd.get_dummies(df_all_songs.drop(columns='Title'))\n",
    "scaled_df_all_songs_ohe = scaler.fit_transform(df_all_songs_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:29.27634Z",
     "iopub.status.busy": "2022-07-04T01:16:29.276041Z",
     "iopub.status.idle": "2022-07-04T01:16:29.283156Z",
     "shell.execute_reply": "2022-07-04T01:16:29.282198Z",
     "shell.execute_reply.started": "2022-07-04T01:16:29.276311Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_similar_song(title, artist):\n",
    "    title = title.lower()\n",
    "    # get the vector for the requested song\n",
    "    song_idx = df_all_songs.query(f\"Title == '{title}' and Artist == '{artist}'\").index.values[0]\n",
    "    song_vector = scaled_df_all_songs_ohe[song_idx]\n",
    "    # find the most similar song\n",
    "    min_difference = 1\n",
    "    closest_song_idx = 0\n",
    "    for index, song in enumerate(scaled_df_all_songs_ohe):\n",
    "        distance = spatial.distance.cosine(song_vector, song)\n",
    "        if distance < min_difference:\n",
    "            if index == song_idx:\n",
    "                pass\n",
    "            else:\n",
    "                min_difference = distance\n",
    "                closest_song_idx = index #np_iterator.index\n",
    "    # get the title and the artist of the most similar song\n",
    "    closest_song = df_all_songs.loc[closest_song_idx,['Title', 'Artist']]\n",
    "    print(\"Closest Song:\\n-------------\", closest_song, sep=\"\\n\")\n",
    "    return closest_song_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:29.284585Z",
     "iopub.status.busy": "2022-07-04T01:16:29.284296Z",
     "iopub.status.idle": "2022-07-04T01:16:31.85726Z",
     "shell.execute_reply": "2022-07-04T01:16:31.856136Z",
     "shell.execute_reply.started": "2022-07-04T01:16:29.284558Z"
    }
   },
   "outputs": [],
   "source": [
    "get_most_similar_song(\"Numb\", \"Linkin Park\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:31.859578Z",
     "iopub.status.busy": "2022-07-04T01:16:31.858924Z",
     "iopub.status.idle": "2022-07-04T01:16:32.767205Z",
     "shell.execute_reply": "2022-07-04T01:16:32.765991Z",
     "shell.execute_reply.started": "2022-07-04T01:16:31.859526Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT Artist, Title FROM df_table WHERE Artist LIKE 'Radio%'\").distinct().show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-04T01:16:32.769022Z",
     "iopub.status.busy": "2022-07-04T01:16:32.768532Z",
     "iopub.status.idle": "2022-07-04T01:16:35.337374Z",
     "shell.execute_reply": "2022-07-04T01:16:35.336455Z",
     "shell.execute_reply.started": "2022-07-04T01:16:32.768972Z"
    }
   },
   "outputs": [],
   "source": [
    "get_most_similar_song(\"creep\", \"Radiohead\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
